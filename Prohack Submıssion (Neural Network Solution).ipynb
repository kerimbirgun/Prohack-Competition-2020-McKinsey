{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, QuantileTransformer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, QuantileTransformer\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor  \n",
    "from keras import regularizers\n",
    "from keras.constraints import Constraint\n",
    "from keras.constraints import max_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(890, 183)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#INITTIATE COLUMNS\n",
    "train = pd.read_csv('prohack_train.csv')\n",
    "df_na = pd.DataFrame(train.isnull().sum(axis = 0))\n",
    "df_na[0] = df_na / train['y'].count()\n",
    "df_na = df_na.rename(columns={0: \"Missing_Rate\"})\n",
    "drops = df_na[df_na['Missing_Rate'] <= 0.5].reset_index()\n",
    "columns_left = drops['index'].values.tolist()\n",
    "\n",
    "#columns_left.append('Gender Inequality Index (GII)')\n",
    "#columns_left.append('Interstellar Data Net users, total (% of population)')\n",
    "#columns_left.append('Estimated gross galactic income per capita, female')\n",
    "#columns_left.append('Old age dependency ratio (old age (65 and older) per 100 creatures (ages 15-64))')\n",
    "\n",
    "df2 = pd.read_csv('prohack_test.csv')\n",
    "columns_left.remove('y')\n",
    "df2 = df2[columns_left]\n",
    "\n",
    "imp = IterativeImputer(max_iter=50, random_state=0)\n",
    "exclude = ['galaxy'] \n",
    "imp.fit(df2.loc[:, df2.columns.difference(exclude)])\n",
    "IterativeImputer(random_state=0)\n",
    "df2.loc[:, df2.columns.difference(exclude)] = imp.transform(df2.loc[:, df2.columns.difference(exclude)])\n",
    "\n",
    "cat_feats = ['galaxy']\n",
    "test = pd.DataFrame(pd.get_dummies(df2,columns=cat_feats,drop_first=True))\n",
    "\n",
    "#drop_list = ['galactic year_1016064.0']\n",
    "#test = test.drop(drop_list,axis=1)\n",
    "test_columns = test.columns.values.tolist()\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2 = pd.read_csv('prohack_train_R_Cleaned_Clustered_4.csv',encoding='latin-1')\n",
    "#df = df2.drop(['Unnamed: 0'],axis=1)\n",
    "#df['Multiplied'] = (df['Old.age.dependency.ratio..old.age..65.and.older..per.100.creatures..ages.15.64..']) * (df['Interstellar.Data.Net.users..total....of.population.']) * (df['Estimated.gross.galactic.income.per.capita..female']) / (df['Gender.Inequality.Index..GII.'])\n",
    "#drop_list = ['y']\n",
    "\n",
    "df2 = pd.read_csv('prohack_train.csv')\n",
    "columns_left.append('y')\n",
    "df2 = df2[columns_left]\n",
    "\n",
    "\n",
    "drop_galaxy = ['Andromeda XII','Andromeda XIX[60]','Andromeda XVIII[60]', 'Andromeda XXII[57]', 'Andromeda XXIV',\n",
    "               'Hercules Dwarf', 'NGC 5253','Triangulum Galaxy (M33)','Tucana Dwarf'] \n",
    "df2 = df2[~df2['galaxy'].isin(drop_galaxy)]\n",
    "\n",
    "#Anomalies\n",
    "#df2 = df2[df2['existence expectancy index']>0.227]\n",
    "#df2 = df2[df2['existence expectancy at birth']>34.25]\n",
    "#df2 = df2[(df2['Gross income per capita']<150000) and (df2['Gross income per capita']>0) ]\n",
    "#df2 = df2[df2['Expected years of education (galactic years)']<26.7]\n",
    "\n",
    "imp = IterativeImputer(max_iter=50, random_state=0)\n",
    "exclude = ['galaxy','y'] \n",
    "imp.fit(df2.loc[:, df2.columns.difference(exclude)])\n",
    "IterativeImputer(random_state=0)\n",
    "df2.loc[:, df2.columns.difference(exclude)] = imp.transform(df2.loc[:, df2.columns.difference(exclude)])\n",
    "\n",
    "df = df2[df2['y'] < 0.5]\n",
    "cat_feats = ['galaxy']\n",
    "df = pd.DataFrame(pd.get_dummies(df,columns=cat_feats,drop_first=True))\n",
    "\n",
    "test_columns.append('y')\n",
    "df2 = df[test_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropping 'galactic year' at index: 0\n",
      "dropping 'existence expectancy at birth' at index: 1\n",
      "dropping 'existence expectancy index' at index: 0\n",
      "dropping 'Population using at least basic drinking-water services (%)' at index: 7\n",
      "dropping 'Intergalactic Development Index (IDI)' at index: 4\n",
      "dropping 'Income Index' at index: 1\n",
      "dropping 'Education Index' at index: 3\n",
      "Remaining variables:\n",
      "Index(['Gross income per capita',\n",
      "       'Expected years of education (galactic years)',\n",
      "       'Mean years of education (galactic years)',\n",
      "       'Intergalactic Development Index (IDI), Rank',\n",
      "       'Population using at least basic sanitation services (%)',\n",
      "       'galaxy_Andromeda I', 'galaxy_Andromeda II', 'galaxy_Andromeda III',\n",
      "       'galaxy_Andromeda IX', 'galaxy_Andromeda V',\n",
      "       ...\n",
      "       'galaxy_UGCA 438 (ESO 407-018)', 'galaxy_UGCA 86', 'galaxy_UGCA 92',\n",
      "       'galaxy_Ursa Major I Dwarf (UMa I dSph)', 'galaxy_Ursa Major II Dwarf',\n",
      "       'galaxy_Ursa Minor Dwarf', 'galaxy_Virgo I', 'galaxy_Willman 1',\n",
      "       'galaxy_Wolf-Lundmark-Melotte (WLM, DDO 221)', 'y'],\n",
      "      dtype='object', length=177)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  3,   5,   6,   9,  11,  12,  13,  14,  15,  16,  17,  18,  19,\n",
       "        20,  21,  22,  23,  24,  25,  26,  27,  28,  29,  30,  31,  32,\n",
       "        33,  34,  35,  36,  37,  38,  39,  40,  41,  42,  43,  44,  45,\n",
       "        46,  47,  48,  49,  50,  51,  52,  53,  54,  55,  56,  57,  58,\n",
       "        59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  71,\n",
       "        72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,  84,\n",
       "        85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
       "        98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110,\n",
       "       111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123,\n",
       "       124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136,\n",
       "       137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149,\n",
       "       150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162,\n",
       "       163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175,\n",
       "       176, 177, 178, 179, 180, 181, 182, 183])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### VIF REMOVALS\n",
    "def calculate_vif_(X, thresh=100):\n",
    "    cols = X.columns\n",
    "    variables = np.arange(X.shape[1])\n",
    "    dropped=True\n",
    "    while dropped:\n",
    "        dropped=False\n",
    "        c = X[cols[variables]].values\n",
    "        vif = [variance_inflation_factor(c, ix) for ix in np.arange(c.shape[1])]\n",
    "\n",
    "        maxloc = vif.index(max(vif))\n",
    "        if max(vif) > thresh:\n",
    "            print('dropping \\'' + X[cols[variables]].columns[maxloc] + '\\' at index: ' + str(maxloc))\n",
    "            variables = np.delete(variables, maxloc)\n",
    "            dropped=True\n",
    "\n",
    "    print('Remaining variables:')\n",
    "    print(X.columns[variables])\n",
    "    return variables\n",
    "    \n",
    "calculate_vif_(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3660, 183)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#VIF_DROPS = ['galactic year','existence expectancy at birth','existence expectancy index','Population using at least basic drinking-water services (%)',\n",
    "            #'Intergalactic Development Index (IDI)','Income Index','Education Index']\n",
    "#df2 = df2.drop(VIF_DROPS,axis=1)\n",
    "X = df2.drop('y',axis=1)\n",
    "y = df2['y']\n",
    "y = np.array(y) ## SEN BU KISMI SIL\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=25)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best test_error    0.0061\n",
      "Median test error    0.0061\n",
      "Loop Finished %    0.03\n",
      "\n",
      "best test_error    0.0061\n",
      "Median test error    0.0066\n",
      "Loop Finished %    0.06\n",
      "\n",
      "best test_error    0.0061\n",
      "Median test error    0.0072\n",
      "Loop Finished %    0.09\n",
      "\n",
      "best test_error    0.0058\n",
      "Median test error    0.0066\n",
      "Loop Finished %    0.12\n",
      "\n",
      "best test_error    0.0058\n",
      "Median test error    0.0072\n",
      "Loop Finished %    0.16\n",
      "\n",
      "best test_error    0.0058\n",
      "Median test error    0.0072\n",
      "Loop Finished %    0.19\n",
      "\n",
      "best test_error    0.0058\n",
      "Median test error    0.0072\n",
      "Loop Finished %    0.22\n",
      "\n",
      "best test_error    0.0058\n",
      "Median test error    0.0073\n",
      "Loop Finished %    0.25\n",
      "\n",
      "best test_error    0.0058\n",
      "Median test error    0.0075\n",
      "Loop Finished %    0.28\n",
      "\n",
      "best test_error    0.0058\n",
      "Median test error    0.0075\n",
      "Loop Finished %    0.31\n",
      "\n",
      "best test_error    0.0058\n",
      "Median test error    0.0075\n",
      "Loop Finished %    0.34\n",
      "\n",
      "best test_error    0.0058\n",
      "Median test error    0.0076\n",
      "Loop Finished %    0.38\n",
      "\n",
      "best test_error    0.0058\n",
      "Median test error    0.0078\n",
      "Loop Finished %    0.41\n",
      "\n",
      "best test_error    0.0058\n",
      "Median test error    0.0079\n",
      "Loop Finished %    0.44\n",
      "\n",
      "best test_error    0.0058\n",
      "Median test error    0.008\n",
      "Loop Finished %    0.47\n",
      "\n",
      "best test_error    0.0058\n",
      "Median test error    0.0081\n",
      "Loop Finished %    0.5\n",
      "\n",
      "best test_error    0.0058\n",
      "Median test error    0.0082\n",
      "Loop Finished %    0.53\n",
      "\n",
      "best test_error    0.0058\n",
      "Median test error    0.0081\n",
      "Loop Finished %    0.56\n",
      "\n",
      "best test_error    0.0058\n",
      "Median test error    0.008\n",
      "Loop Finished %    0.59\n",
      "\n",
      "best test_error    0.0058\n",
      "Median test error    0.0079\n",
      "Loop Finished %    0.62\n",
      "\n",
      "best test_error    0.0058\n",
      "Median test error    0.008\n",
      "Loop Finished %    0.66\n",
      "\n",
      "best test_error    0.0058\n",
      "Median test error    0.0079\n",
      "Loop Finished %    0.69\n",
      "\n",
      "best test_error    0.0058\n",
      "Median test error    0.0078\n",
      "Loop Finished %    0.72\n",
      "\n",
      "best test_error    0.0058\n",
      "Median test error    0.0079\n",
      "Loop Finished %    0.75\n",
      "\n",
      "best test_error    0.0058\n",
      "Median test error    0.0078\n",
      "Loop Finished %    0.78\n",
      "\n",
      "best test_error    0.0058\n",
      "Median test error    0.0077\n",
      "Loop Finished %    0.81\n",
      "\n",
      "best test_error    0.0058\n",
      "Median test error    0.0076\n",
      "Loop Finished %    0.84\n",
      "\n",
      "best test_error    0.0058\n",
      "Median test error    0.0075\n",
      "Loop Finished %    0.88\n",
      "\n",
      "best test_error    0.0058\n",
      "Median test error    0.0076\n",
      "Loop Finished %    0.91\n",
      "\n",
      "best test_error    0.0058\n",
      "Median test error    0.0075\n",
      "Loop Finished %    0.94\n",
      "\n",
      "best test_error    0.0058\n",
      "Median test error    0.0076\n",
      "Loop Finished %    0.97\n",
      "\n",
      "best test_error    0.0058\n",
      "Median test error    0.0077\n",
      "Loop Finished %    1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from keras import backend as K\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "\n",
    "batch_list = [50,100] \n",
    "#quantile_list = [5,10] qn,quantile_list,\n",
    "first_layer_list = [1,2]\n",
    "second_layer_list = [1,2] \n",
    "dropout_input = [0.1,0.2]\n",
    "dropout_hidden_1 = [0.1,0.2]\n",
    "dropout_hidden_2 = [0.1]\n",
    "patience_list = [15]\n",
    "train_error = []\n",
    "test_error = []\n",
    "batch = []\n",
    "first_layer = []\n",
    "second_layer = []\n",
    "third_layer = []\n",
    "quantile = []\n",
    "drop_input = []\n",
    "drop_hidden1 = []\n",
    "drop_hidden2 = []\n",
    "patience_bracket = []\n",
    "\n",
    "for  fl,sl,aa,drop_i,drop_h1,droph2,pt in itertools.product(first_layer_list, \n",
    "    second_layer_list,batch_list,dropout_input,dropout_hidden_1,dropout_hidden_2,patience_list):\n",
    "    \n",
    "                K.clear_session()\n",
    "                #qt = QuantileTransformer(n_quantiles=qn)\n",
    "                #X_train = qt.fit_transform(X_train)\n",
    "                #X_test = qt.fit_transform(X_test)\n",
    "                \n",
    "                early_stop = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 0, patience = pt)\n",
    "\n",
    "                # Create model and fit data to it\n",
    "                model = Sequential()\n",
    "\n",
    "                # Input Layer\n",
    "                model.add(Dense(X.shape[1], input_shape=(X.shape[1],), activation = 'relu'))\n",
    "                model.add(Dropout(drop_i))\n",
    "\n",
    "                # Hidden Layers\n",
    "                model.add(Dense(X.shape[1]/fl, activation = 'relu'))\n",
    "                model.add(Dropout(drop_h1))\n",
    "\n",
    "                model.add(Dense(X.shape[1]/sl, activation = 'softplus'))\n",
    "                model.add(Dropout(droph2))\n",
    "                \n",
    "                # Output Layer\n",
    "                model.add(Dense(1, activation = 'softplus'))\n",
    "\n",
    "                model.compile(optimizer = 'adam', loss = 'mse',learning_rate=0.0001)\n",
    "\n",
    "                model.fit(X_train,y_train,epochs = 1000, validation_data = (X_test, y_test), callbacks=[early_stop],\n",
    "                          batch_size=aa,verbose = 0)\n",
    "\n",
    "                pred_train = model.predict(X_train)\n",
    "                pred_test = model.predict(X_test)\n",
    "\n",
    "                train_error1 = np.sqrt(mean_squared_error(y_train, pred_train))\n",
    "                test_error1 = np.sqrt(mean_squared_error(y_test, pred_test))\n",
    "\n",
    "                batch.append(aa)\n",
    "                test_error.append(test_error1)\n",
    "                train_error.append(train_error1)\n",
    "                first_layer.append(fl)\n",
    "                second_layer.append(sl)\n",
    "                #third_layer.append(tl)\n",
    "                #quantile_list.append(qn)\n",
    "                drop_input.append(drop_i)\n",
    "                drop_hidden1.append(drop_h1)\n",
    "                drop_hidden2.append(droph2)\n",
    "                patience_bracket.append(pt)\n",
    "                print(pd.DataFrame(test_error,columns=['best test_error']).sort_values(by='best test_error').min().round(4).to_string())\n",
    "                print(pd.DataFrame(test_error,columns=['Median test error']).sort_values(by='Median test error').median().round(4).to_string())\n",
    "                print((pd.DataFrame(test_error,columns=['Loop Finished %']).sort_values(by='Loop Finished %').count() / 32).round(2).to_string())\n",
    "                print()\n",
    "                \n",
    "                K.clear_session()\n",
    "                \n",
    "                if test_error1 == min(test_error):\n",
    "                    model.save(\"Prohack_Model_With\" + str(test_error1))\n",
    "                else:\n",
    "                    None\n",
    "   \n",
    "                del model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch size</th>\n",
       "      <th>first layer divisor</th>\n",
       "      <th>second layer divisor</th>\n",
       "      <th>Input Dropout</th>\n",
       "      <th>Hidden 1 Dropout</th>\n",
       "      <th>Hidden 2 Dropout</th>\n",
       "      <th>Patience</th>\n",
       "      <th>train_error</th>\n",
       "      <th>test_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.004172</td>\n",
       "      <td>0.005790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.004172</td>\n",
       "      <td>0.006114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.004374</td>\n",
       "      <td>0.006412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    batch size  first layer divisor  second layer divisor  Input Dropout  \\\n",
       "3           50                    1                     1            0.2   \n",
       "0           50                    1                     1            0.1   \n",
       "19          50                    2                     1            0.2   \n",
       "\n",
       "    Hidden 1 Dropout  Hidden 2 Dropout  Patience  train_error  test_error  \n",
       "3                0.2               0.1        15     0.004172    0.005790  \n",
       "0                0.1               0.1        15     0.004172    0.006114  \n",
       "19               0.2               0.1        15     0.004374    0.006412  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After completing the entire loop, assess performance results\n",
    "b = pd.DataFrame(batch,columns=['batch size'])\n",
    "e = pd.DataFrame(test_error,columns=['test_error'])\n",
    "t = pd.DataFrame(train_error,columns=['train_error'])\n",
    "fl = pd.DataFrame(first_layer,columns=['first layer divisor'])\n",
    "sl = pd.DataFrame(second_layer,columns=['second layer divisor'])\n",
    "tl = pd.DataFrame(third_layer,columns=['third layer divisor'])\n",
    "#qn = pd.DataFrame(quantile_list,columns=['Scaling Quantile'])\n",
    "di = pd.DataFrame(drop_input,columns=['Input Dropout'])\n",
    "dh1 = pd.DataFrame(drop_hidden1,columns=['Hidden 1 Dropout'])\n",
    "dh2 = pd.DataFrame(drop_hidden2,columns=['Hidden 2 Dropout'])\n",
    "pt = pd.DataFrame(patience_bracket,columns=['Patience'])\n",
    "\n",
    "results = pd.concat([b,fl,sl,di,dh1,dh2,pt,t,e],axis=1)\n",
    "#results.to_csv('grid_search_5.csv')\n",
    "results.sort_values(by='test_error').head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REAL PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_transform = scaler.transform(test)\n",
    "preds = np.array(df_pred_transform, dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model load \n",
    "from tensorflow.keras.models import load_model\n",
    "model_00496 = load_model('Prohack_Model_With0.00496748783373051') \n",
    "result = model_00496.predict(preds)\n",
    "result = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALLOCATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = result\n",
    "pot_inc = -np.log(index+0.01)+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2= pot_inc**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result.reset_index()\n",
    "\n",
    "ss = pd.DataFrame({\n",
    "    'Index':result.index,\n",
    "    'pred': result[0],\n",
    "    'opt_pred':0,\n",
    "    'eei':test['existence expectancy index'], # So that we can split into low and high EEI galaxies\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:5303: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "p2.columns = ['index']\n",
    "ss.loc[p2.nlargest(400,'index').index, 'opt_pred']=100\n",
    "ss=ss.sort_values('pred')\n",
    "ss.iloc[400:600].opt_pred = 50\n",
    "ss=ss.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "increase = (ss['opt_pred']*p2['index'])/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1855.6131571769688 6400 50000\n"
     ]
    }
   ],
   "source": [
    "print(sum(increase), ss.loc[ss.eei < 0.7, 'opt_pred'].sum(), ss['opt_pred'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss[['Index', 'pred', 'opt_pred']].to_csv('submission_6_21_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2ND VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Index','eei','pred']\n",
    "test_sub = ss[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def increase_coef(y):\n",
    "    potential = 3 - np.log(0.01 + y)\n",
    "    return (potential**2)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>eei</th>\n",
       "      <th>pred</th>\n",
       "      <th>likely_increase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.456086</td>\n",
       "      <td>0.040942</td>\n",
       "      <td>0.035725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.529835</td>\n",
       "      <td>0.036014</td>\n",
       "      <td>0.036952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.560976</td>\n",
       "      <td>0.035949</td>\n",
       "      <td>0.036969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.565910</td>\n",
       "      <td>0.038315</td>\n",
       "      <td>0.036361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.588274</td>\n",
       "      <td>0.023475</td>\n",
       "      <td>0.040921</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index       eei      pred  likely_increase\n",
       "0      0  0.456086  0.040942         0.035725\n",
       "1      1  0.529835  0.036014         0.036952\n",
       "2      2  0.560976  0.035949         0.036969\n",
       "3      3  0.565910  0.038315         0.036361\n",
       "4      4  0.588274  0.023475         0.040921"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sub['likely_increase'] = increase_coef(test_sub.pred)\n",
    "test_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank error estimation: 124\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "## rank error estimation\n",
    "abs_rank_error = int(0.14*len(test))\n",
    "print(\"Rank error estimation:\", abs_rank_error)\n",
    "\n",
    "## norm distribution\n",
    "opt_pred = [100*norm.cdf((499 - x)/abs_rank_error) for x in range(len(test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6291.296801840401, 49947.3079854035)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_dist = test_sub[['Index', 'likely_increase']].set_index('Index')\\\n",
    "    .sort_values('likely_increase', ascending=False)\n",
    "opt_dist['opt_pred'] = opt_pred\n",
    "test_sub['opt_pred'] = test_sub['Index'].map(opt_dist.opt_pred)\n",
    "\n",
    "test_sub.loc[test_sub.eei < 0.7, 'opt_pred'].sum(), test_sub.opt_pred.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1848.114288402057"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(test_sub['opt_pred'] * test_sub['likely_increase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>pred</th>\n",
       "      <th>opt_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.040942</td>\n",
       "      <td>90.834710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.036014</td>\n",
       "      <td>98.434953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.035949</td>\n",
       "      <td>98.527535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.038315</td>\n",
       "      <td>96.063173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.023475</td>\n",
       "      <td>99.980018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index      pred   opt_pred\n",
       "0      0  0.040942  90.834710\n",
       "1      1  0.036014  98.434953\n",
       "2      2  0.035949  98.527535\n",
       "3      3  0.038315  96.063173\n",
       "4      4  0.023475  99.980018"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sub = test_sub.sort_values(by='Index').reset_index(drop=True)\n",
    "test_sub[['Index', 'pred', 'opt_pred']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sub[['Index', 'pred', 'opt_pred']].to_csv('submission_6_21_1.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
